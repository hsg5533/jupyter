{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b960ed61-2936-4ca0-9451-5da3c89e8d66",
   "metadata": {},
   "source": [
    "# NLTK 자연어 처리 패키지¶\n",
    "- NLTK(Natural Language Toolkit) 패키지는 교육용으로 개발된 자연어 처리 및 문서 분석용 파이썬 패키지다. 다양한 기능 및 예제를 가지고 있으며 실무 및 연구에서도 많이 사용된다.\n",
    "\n",
    "- NLTK 패키지가 제공하는 주요 기능은 다음과 같다.\n",
    "\n",
    "    - 말뭉치\n",
    "\n",
    "    - 토큰 생성\n",
    "\n",
    "    - 형태소 분석\n",
    "\n",
    "    - 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b6c15-6a39-4801-b01a-960e05a4e346",
   "metadata": {},
   "source": [
    "# 말뭉치\n",
    "- 말뭉치(corpus)는 자연어 분석 작업을 위해 만든 샘플 문서 집합을 말한다. \n",
    "- 단순히 소설, 신문 등의 문서를 모아놓은 것도 있지만 품사, 형태소, 등의 보조적 의미를 추가하고 쉬운 분석을 위해 구조적인 형태로 정리해 놓은 것을 포함한다. \n",
    "- NLTK 패키지의 corpus 서브패키지에서는 다양한 연구용 말뭉치를 제공한다. \n",
    "- 이 목록은 전체 corpus의 일부일 뿐이다. \n",
    "- 말뭉치 자료는 설치시에 제공되지 않고 download 명령으로 사용자가 다운로드 받아야 한다. \n",
    "- nltk.download(\"book\") 명령을 실행하면 NLTK 패키지 사용자 설명서에서 요구하는 대부분의 말뭉치를 다운로드 받아준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa9053-028d-47c9-8205-d2604c356df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cffed-ea5b-4d74-803b-c40605a79546",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"book\", quiet=True)\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9bd0d-c69f-4f8a-b09f-fcd619d76faf",
   "metadata": {},
   "source": [
    "- 저작권이 말소된 문학작품을 포함하는 gutenberg 말뭉치에는 다음과 같은 작품이 샘플로 포함되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534d01c-e5a2-4754-851c-1c79cc7d0b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd2f31-a2cf-4b6c-9339-49333b452f92",
   "metadata": {},
   "source": [
    "- 이 중 제인 오스틴의 엠마 문서를 살펴보면 다음과 같이 원문 형태 그대로를 포함하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e8035-ecc4-44ad-94f5-e9122c1e15a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emma_raw = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "print(emma_raw[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d1074-4e1b-4211-8b04-c604b99d3f2f",
   "metadata": {},
   "source": [
    "# 토큰 생성¶\n",
    "- 자연어 문서를 분석하기 위해서는 우선 긴 문자열을 분석을 위한 작은 단위로 나누어야 한다. \n",
    "- 이 문자열 단위를 토큰(token)이라고 하고 이렇게 문자열을 토큰으로 나누는 작업을 토큰 생성(tokenizing)이라고 한다. \n",
    "- 영문의 경우에는 문장, 단어 등을 토큰으로 사용하거나 정규 표현식을 쓸 수 있다.\n",
    "\n",
    "- 문자열을 토큰으로 분리하는 함수를 토큰 생성 함수(tokenizer)라고 한다. \n",
    "- 토큰 생성 함수는 문자열을 입력받아 토큰 문자열의 리스트를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f5aaf6-b66c-4aaa-8d92-849c750b01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(emma_raw[:10000])[10]) #문장 단위 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4164a-ad20-44c6-932b-10acedcce8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(emma_raw[50:100]) #단어 단위 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac8ffc-3ff0-4cd8-a25b-69dc05b4209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "retokenize = RegexpTokenizer(\"[\\w]+\")\n",
    "retokenize.tokenize(emma_raw[50:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7d1cb-59df-44f6-8a01-19c0e0542c60",
   "metadata": {},
   "source": [
    "# 형태소 분석¶\n",
    "- 형태소(morpheme)는 언어학에서 일정한 의미가 있는 가장 작은 말의 단위를 뜻한다. \n",
    "- 보통 자연어 처리에서는 토큰으로 형태소를 이용한다. \n",
    "- 형태소 분석(morphological analysis)이란 단어로부터 어근, 접두사, 접미사, 품사 등 다양한 언어적 속성을 파악하고 \n",
    "  이를 이용하여 형태소를 찾아내거나 처리하는 작업이다. \n",
    "- 형태소 분석의 예로는 다음과 같은 작업이 있다.\n",
    "\n",
    "    - 어간 추출(stemming)\n",
    "\n",
    "    - 원형 복원(lemmatizing)\n",
    "\n",
    "    - 품사 부착(Part-Of-Speech tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554efb9-af35-44a7-be59-b759e236676d",
   "metadata": {},
   "source": [
    "# 어간 추출과 원형 복원¶\n",
    "- 어간 추출(stemming)은 변화된 단어의 접미사나 어미를 제거하여 같은 의미를 가지는 형태소의 기본형을 찾는 방법이다. \n",
    "- NLTK는 PorterStemmer LancasterStemmer 등을 제공한다. \n",
    "- 어간 추출법은 단순히 어미를 제거할 뿐이므로 단어의 원형의 정확히 찾아주지는 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0013569-6859-48e6-8aa9-e71e2b7cfbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "st1 = PorterStemmer()\n",
    "st2 =  LancasterStemmer()\n",
    "\n",
    "words = [\"fly\", \"flies\", \"flying\", \"flew\", \"flown\"]\n",
    "\n",
    "print(\"Porter Stemmer   :\", [st1.stem(w) for w in words])\n",
    "print(\"Lancaster Stemmer:\", [st2.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21700c17-ff70-4060-bc83-1e14c29afc03",
   "metadata": {},
   "source": [
    "- 원형 복원(lemmatizing)은 같은 의미를 가지는 여러 단어를 사전형으로 통일하는 작업이다. 품사(part of speech)를 \n",
    "지정하는 경우 좀 더 정확한 원형을 찾을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda3d4c-587d-402a-b349-ca65d3f34f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "[lm.lemmatize(w, pos=\"v\") for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018883bd-9205-433c-8074-20cd79ac4709",
   "metadata": {},
   "source": [
    "# 품사 부착¶\n",
    "- 품사(POS, part-of-speech)는 낱말을 문법적인 기능이나 형태, 뜻에 따라 구분한 것이다. \n",
    "- 품사의 구분은 언어마다 그리고 학자마다 다르다. \n",
    "- 예를 들어 NLTK에서는 펜 트리뱅크 태그세트(Penn Treebank Tagset)라는 것을 이용한다. \n",
    "- 다음은 펜 트리뱅크 태그세트에서 사용하는 품사의 예이다.\n",
    "\n",
    "    - NNP: 단수 고유명사\n",
    "    - VB: 동사\n",
    "    - VBP: 동사 현재형\n",
    "    - TO: to 전치사\n",
    "    - NN: 명사(단수형 혹은 집합형)\n",
    "    - DT: 관형사\n",
    "\n",
    "- nltk.help.upenn_tagset 명령으로 자세한 설명을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca71443-7805-4e5b-aaec-5fc342e547b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset(\"VB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e4880-41a5-4a5f-89df-9dfe477311e2",
   "metadata": {},
   "source": [
    "- pos_tag 명령을 사용하면 단어 토큰에 품사를 부착하여 튜플로 출력한다. \n",
    "- 다음 예문에서 refuse, permit이라는 같은 철자의 단어가 각각 동사와 명사로 다르게 품사 부착된 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4bf32-875e-4726-abee-6d5dcaf40694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "sentence = \"Emma refused to permit us to obtain the refuse permit\"\n",
    "tagged_list = pos_tag(word_tokenize(sentence))\n",
    "tagged_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1f8b9-3fc0-4ed1-aa7b-3e64ef035771",
   "metadata": {},
   "source": [
    "- 품사 태그 정보를 사용하면 명사인 토큰만 선택할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ec18d-d8ff-456a-9c99-e64f4dcf08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_list = [t[0] for t in tagged_list if t[1] == \"NN\"]\n",
    "nouns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c1db5-38d2-425d-9ab0-e06ec0fa0097",
   "metadata": {},
   "source": [
    "- untag 명령을 사용하면 태그 튜플을 제거할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8917257-9c41-4f40-adc5-9681654d3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import untag\n",
    "untag(tagged_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827cf0b-d504-473f-af2f-1775cb9a74ee",
   "metadata": {},
   "source": [
    "# Text 클래스\n",
    "- NLTK의 Text 클래스는 문서 분석에 유용한 여러가지 메서드를 제공한다. \n",
    "- 토큰열을 입력하여 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27792f13-0b5d-4694-9e2e-fb5148d43e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text\n",
    "\n",
    "text = Text(retokenize.tokenize(emma_raw))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dda803-4c9e-4d0f-bf79-b2fa5f675f17",
   "metadata": {},
   "source": [
    "- plot 메소드를 사용하면 각 단어(토큰)의 사용 빈도를 그래프로 그려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a6440-6aff-4fb3-bf51-f423b81356dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "text.plot(20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17eca9-3b0e-499c-93a5-7fc3fcb789da",
   "metadata": {},
   "source": [
    "- dispersion_plot 메서드는 단어가 사용된 위치를 시각화한다. \n",
    "- 소설 엠마의 각 등장인물에 대해 적용하면 다음과 같은 결과를 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7133330-354e-4e0e-a8a6-a2101d37c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.dispersion_plot([\"Emma\", \"Knightley\", \"Frank\", \"Jane\", \"Harriet\", \"Robert\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969c38f-f72f-4f40-ad56-f0df57c48ed9",
   "metadata": {},
   "source": [
    "- concordance 메서드로 단어가 사용된 위치를 직접 표시하면 문맥(context)이 어떤지 볼 수 있다. \n",
    "- 여기에서 문맥은 해당 단어의 앞과 뒤에 사용된 단어를 뜻한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1b5b1-bbb7-4897-974c-4714b365c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.concordance(\"Emma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0eaa04-ef4c-4025-a65a-8ed631f97158",
   "metadata": {},
   "source": [
    "- similar 메서드는 같은 문맥에서 주어진 단어 대신 사용된 횟수가 높은 단어들을 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8733917-3491-49e7-a8bd-8ccdc9c9ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.similar(\"Emma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280159d4-2ac5-4a4c-b2ef-2bb6247e3e37",
   "metadata": {},
   "source": [
    "- 두 단어의 공통 문맥을 보려면 common_contexts 메서드를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4eb197-282f-44a5-9735-d43cacf03983",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.common_contexts([\"Emma\", \"she\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2329d4-7a4a-4ede-b4e6-ed6d31daea5f",
   "metadata": {},
   "source": [
    "# FreqDist\n",
    "- FreqDist 클래스는 문서에 사용된 단어(토큰)의 사용빈도 정보를 담는 클래스이다. \n",
    "- Text 클래스의 vocab 메서드로 추출할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627943e9-467c-4dfd-9f45-8e2c9d2fa1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = text.vocab()\n",
    "type(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2442db9-b55f-48e1-ae3c-fe814f69c50b",
   "metadata": {},
   "source": [
    "- 또는 다음처럼 토큰 리스트를 넣어서 직업 만들 수도 있다. \n",
    "- 다음 코드에서는 Emma 말뭉치에서 사람의 이름만 모아서 FreqDist 클래스 객체를 만들었다. \n",
    "- 품사 태그에서 NNP(고유대명사)이면서 필요없는 단어(stop words)는 제거하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa75ed2-322b-4482-adbf-5cd7b41ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "stopwords = [\"Mr.\", \"Mrs.\", \"Miss\", \"Mr\", \"Mrs\", \"Dear\"]\n",
    "emma_tokens = pos_tag(retokenize.tokenize(emma_raw))\n",
    "names_list = [t[0] for t in emma_tokens if t[1] == \"NNP\" and t[0] not in stopwords]\n",
    "fd_names = FreqDist(names_list)\n",
    "print(fd_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e602af5-02a9-4fd6-b6a2-694fc9a741f8",
   "metadata": {},
   "source": [
    "- FreqDist 클래스는 단어를 키(key), 출현빈도를 값(value)으로 가지는 사전 자료형과 유사하다. \n",
    "- 다음 코드는 전체 단어의 수, “Emma”라는 단어의 출현 횟수, 확률을 각각 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68bc52-91da-46d3-aebd-fe441b1c7435",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_names.N(), fd_names[\"Emma\"], fd_names.freq(\"Emma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d86ab3-2908-4e91-8b1e-74aa2fc5d257",
   "metadata": {},
   "source": [
    "- most_common 메서드를 사용하면 가장 출현 횟수가 높은 단어를 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d4e02-3296-429e-a227-d2abe74dc2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_names.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229779f9-f455-4a3f-9fc9-1863c35876ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 워드클라우드¶\n",
    "- wordcloud 패키지를 사용하면 단어의 사용 빈도수에 따라 워드클라우드(Word Cloud) 시각화를 할 수 있다.\n",
    "# 워드클라우드 설치법¶\n",
    "- 관리자 모드로 콘솔 실행\n",
    "- pip install pillow --upgrade\n",
    "- conda install -c conda-forge wordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de9795-f11b-44ce-924a-24ff69a2e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(width=2560, height=1440, background_color=\"white\", random_state=0)\n",
    "plt.imshow(wc.generate_from_frequencies(fd_names))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
